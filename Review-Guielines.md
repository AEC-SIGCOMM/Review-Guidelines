# Review-Guidelines

## Introduction

Our community has limited experience in such an evaluation, but artefacts are an important part of 
many of the results described in the papers published in our 
conferences. Reviewing artefacts is very different from reviewing 
papers. The first point is that the artefacts that you evaluate have 
been released in papers that have already been accepted by competent TPC 
members and reviewers. This implies that you do not need to provide 
detailed comments on the paper. In most cases, the version of the paper 
that you see is the final version with some additions to describe the 
artefacts.

The focus of your review has to be on the artefacts and their relationship 
with the results described in the paper. We have worked with several 
members of the community to propose an artefacts review template that should 
help you in evaluating the artefacts.

There are very different types of artefacts that can be associated to 
papers. The most common ones are software and datasets, but you might 
see studies with hardware implementations or testbed trials. For 
software, we would ask you to try to download, recompile (if necessary),
and install the software.
This might work perfectly but could also fail due to missing 
libraries and other classical differences between your computer and the 
authors' one. In this case provide information to the authors so 
that they can fix the artefacts. You are probably the first non-author 
who tries to use the artefact. Practical difficulties are possible and 
you might need to discuss with the authors to solve them. This will be 
possible during the rebuttal period once all reviews have been received. 
Once you have installed the software, please try to use it as described 
by the authors. In some cases, this software expects specific hardware, 
if you do not have access to such hardware, let us know and we'll see 
what can be done. All of the above steps are, usually, described in the
documentation that comes along with the software. Such documentation,
may be incomplete, not providing the actual information necessary to
actually, make the software work. You can provide also a feedback to the
authors concerning lacks or unclarities in the documentation. 

Concerning datasets, please try to look at the amount of data that is 
released by the authors, its structure and the associated metadata. The 
latter is very important because it influences the reusability of the 
data collected by the authors. If the authors provided analysis scripts 
that parse the data, please try to reuse them as well. Again, documentation
is key. If the content and format of the data is not clear, its value is somehow
reduced.

Our objective is more to encourage the authors to 
release their artefacts and improve their quality/reusability than 
simply badging them.

## Artefact Definition

## The ACM Badging System

### Artefact Available

### Artefact Functional 

### Artefact Reusable

## Experience since the ACM SIGCOMM Reproducibility Workshop 2017

## Discussion

## Take Away


